{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (v.norm() + eps)\n",
    "\n",
    "class SpectralNorm(nn.Module):\n",
    "    def __init__(self, module, name='weight', power_iterations=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        if not self._made_params():\n",
    "            self._make_params()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + \"_u\")\n",
    "        v = getattr(self.module, self.name + \"_v\")\n",
    "        w = getattr(self.module, self.name + \"_bar\")\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
    "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
    "\n",
    "        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _made_params(self):\n",
    "        try:\n",
    "            u = getattr(self.module, self.name + \"_u\")\n",
    "            v = getattr(self.module, self.name + \"_v\")\n",
    "            w = getattr(self.module, self.name + \"_bar\")\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "\n",
    "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        u.data = l2normalize(u.data)\n",
    "        v.data = l2normalize(v.data)\n",
    "        w_bar = Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "        self.module.register_parameter(self.name + \"_u\", u)\n",
    "        self.module.register_parameter(self.name + \"_v\", v)\n",
    "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)\n",
    "\n",
    "\n",
    "\n",
    "def upconv(in_channels, out_channels, kernel_size, stride=2, padding=2, batch_norm=True, init_zero_weights=True, spectral_norm=False):\n",
    "    \"\"\"Creates a upsample-and-convolution layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "\n",
    "    if stride>1:\n",
    "        layers.append(nn.Upsample(scale_factor=stride))\n",
    "\n",
    "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1, padding=padding, bias=False)\n",
    "\n",
    "    if init_zero_weights:\n",
    "        conv_layer.weight.data = torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.001\n",
    "\n",
    "    if spectral_norm:\n",
    "        layers.append(SpectralNorm(conv_layer))\n",
    "    else:\n",
    "        layers.append(conv_layer)\n",
    "\n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, stride=2, padding=2, batch_norm=True, init_zero_weights=True, spectral_norm=False):\n",
    "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "\n",
    "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "\n",
    "    if init_zero_weights:\n",
    "        conv_layer.weight.data = torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.001\n",
    "            \n",
    "    if spectral_norm:\n",
    "        layers.append(SpectralNorm(conv_layer))\n",
    "    else:\n",
    "        layers.append(conv_layer)\n",
    "\n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, conv_dim):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_layer = conv(in_channels=conv_dim, out_channels=conv_dim, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGenerator(nn.Module):\n",
    "    def __init__(self, noise_size, conv_dim=32, spectral_norm=False):\n",
    "        super(DCGenerator, self).__init__()\n",
    "\n",
    "        self.conv_dim = conv_dim\n",
    "\n",
    "        # self.linear_bn = nn.Sequential(nn.Linear(noise_size, conv_dim*4*4*4), nn.BatchNorm1d(noise_size, conv_dim*4*4*4))\n",
    "        self.linear_bn = upconv(in_channels=noise_size, out_channels=conv_dim*4, kernel_size=2, stride=1, spectral_norm=spectral_norm)\n",
    "        self.upconv1 = upconv(in_channels=conv_dim*4, out_channels=conv_dim*2, kernel_size=3, stride=2, padding=1, spectral_norm=spectral_norm)\n",
    "        self.upconv2 = upconv(in_channels=conv_dim*2, out_channels=conv_dim, kernel_size=3, stride=2, padding=1, spectral_norm=spectral_norm)\n",
    "        self.upconv3 = upconv(in_channels=conv_dim, out_channels=3, kernel_size=3, stride=2, padding=1, batch_norm=False, spectral_norm=spectral_norm)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"Generates an image given a sample of random noise.\n",
    "\n",
    "            Input\n",
    "            -----\n",
    "                z: BS x noise_size x 1 x 1   -->  BSx100x1x1 (during training)\n",
    "\n",
    "            Output\n",
    "            ------\n",
    "                out: BS x channels x image_width x image_height  -->  BSx3x32x32 (during training)\n",
    "        \"\"\"\n",
    "        batch_size = z.size(0)\n",
    "        out = F.relu(self.linear_bn(z)).view(-1, self.conv_dim*4, 4, 4)    # BS x 128 x 4 x 4\n",
    "        out = F.relu(self.upconv1(out))  # BS x 64 x 8 x 8\n",
    "        out = F.relu(self.upconv2(out))  # BS x 32 x 16 x 16\n",
    "        out = torch.tanh(self.upconv3(out))  # BS x 3 x 32 x 32\n",
    "        \n",
    "        out_size = out.size()\n",
    "        if out_size != torch.Size([batch_size, 3, 32, 32]):\n",
    "            raise ValueError(\"expect {} x 3 x 32 x 32, but get {}\".format(batch_size, out_size))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 32, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = DCGenerator(noise_size=100, conv_dim=32)\n",
    "x = torch.rand([4, 100, 1, 1], dtype=torch.float32)\n",
    "g = G.forward(x)\n",
    "g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCDiscriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_dim=32, spectral_norm=False):\n",
    "        super(DCDiscriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = conv(in_channels=3, out_channels=conv_dim, kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
    "        self.conv2 = conv(in_channels=conv_dim, out_channels=conv_dim*2, kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
    "        self.conv3 = conv(in_channels=conv_dim*2, out_channels=conv_dim*4, kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
    "        self.conv4 = conv(in_channels=conv_dim*4, out_channels=1, kernel_size=5, stride=2, padding=1, batch_norm=False, spectral_norm=spectral_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        out = F.relu(self.conv1(x))   # BS x 32 x 16 x 16\n",
    "        out = F.relu(self.conv2(out))    # BS x 64 x 8 x 8\n",
    "        out = F.relu(self.conv3(out))    # BS x 128 x 4 x 4\n",
    "        out = torch.sigmoid(self.conv4(out)).squeeze()    # BS x 1 x 1 x 1\n",
    "        out_size = out.size()\n",
    "\n",
    "        if out_size != torch.Size([batch_size,]):\n",
    "            raise ValueError(\"expect {} x 1, but get {}\".format(batch_size, out_size))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4967, 0.4979, 0.4988, 0.5016], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand([4, 3, 32, 32], dtype=torch.float32)\n",
    "D = DCDiscriminator(32)\n",
    "d = D.forward(x)\n",
    "d.shape\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "\n",
    "                \n",
    "def to_var(tensor, cuda=False):\n",
    "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
    "\n",
    "        Arguments:\n",
    "            tensor: A Tensor object.\n",
    "            cuda: A boolean flag indicating whether to use the GPU.\n",
    "\n",
    "        Returns:\n",
    "            A Variable object, on the GPU if cuda==True.\n",
    "    \"\"\"\n",
    "    if cuda:\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "    \n",
    "def to_data(x):\n",
    "    \"\"\"Converts variable to numpy.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cpu()\n",
    "    return x.data.numpy()\n",
    "\n",
    "\n",
    "def create_dir(directory):\n",
    "    \"\"\"Creates a directory if it doesn't already exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def gan_checkpoint(iteration, G, D, opts):\n",
    "    \"\"\"Saves the parameters of the generator G and discriminator D.\n",
    "    \"\"\"\n",
    "    G_path = os.path.join(opts.checkpoint_dir, f'G_{iteration}.pkl')\n",
    "    D_path = os.path.join(opts.checkpoint_dir, f'D_{iteration}.pkl')\n",
    "    torch.save(G.state_dict(), G_path)\n",
    "    torch.save(D.state_dict(), D_path)\n",
    "\n",
    "def load_checkpoint(opts, iteration):\n",
    "    \"\"\"Loads the generator and discriminator models from checkpoints.\n",
    "    \"\"\"\n",
    "    G_path = os.path.join(opts.load, f'G_{iteration}.pkl')\n",
    "    D_path = os.path.join(opts.load, f'D_{iteration}.pkl')\n",
    "\n",
    "    G = DCGenerator(noise_size=opts.noise_size, conv_dim=opts.g_conv_dim, spectral_norm=opts.spectral_norm)\n",
    "    D = DCDiscriminator(conv_dim=opts.d_conv_dim)\n",
    "\n",
    "    G.load_state_dict(torch.load(G_path, map_location=lambda storage, loc: storage))\n",
    "    D.load_state_dict(torch.load(D_path, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        G.cuda()\n",
    "        D.cuda()\n",
    "        print('Models moved to GPU.')\n",
    "\n",
    "    return G, D\n",
    "\n",
    "\n",
    "def gan_save_samples(G, fixed_noise, iteration, opts):\n",
    "    generated_images = G(fixed_noise)\n",
    "    generated_images = to_data(generated_images)\n",
    "    # save images in sample dir\n",
    "\n",
    "def create_model(opts):\n",
    "    \"\"\"Builds the generators and discriminators.\n",
    "    \"\"\"\n",
    "    ### GAN\n",
    "    G = DCGenerator(noise_size=opts.noise_size, conv_dim=opts.g_conv_dim, spectral_norm=opts.spectral_norm)\n",
    "    D = DCDiscriminator(conv_dim=opts.d_conv_dim, spectral_norm=opts.spectral_norm)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        G.cuda()\n",
    "        D.cuda()\n",
    "        print('Models moved to GPU.')\n",
    "    return G, D\n",
    "\n",
    "def sample_noise(batch_size, dim):\n",
    "    \"\"\"\n",
    "    Generate a PyTorch Tensor of uniform random noise.\n",
    "\n",
    "    Input:\n",
    "    - batch_size: Integer giving the batch size of noise to generate.\n",
    "    - dim: Integer giving the dimension of noise to generate.\n",
    "\n",
    "    Output:\n",
    "    - A PyTorch Tensor of shape (batch_size, dim, 1, 1) containing uniform\n",
    "      random noise in the range (-1, 1).\n",
    "    \"\"\"\n",
    "    return to_var(torch.rand(batch_size, dim) * 2 - 1).unsqueeze(2).unsqueeze(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download link : https://drive.google.com/file/d/1EW93WrocQ6gKXRB28QbT8C-HSKOgWhY1/view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "def rgba_to_rgb(image):\n",
    "    # Remove the alpha channel (assuming it's the fourth channel)\n",
    "    rgb_image = image[:3, :, :]\n",
    "    return rgb_image\n",
    "\n",
    "def get_emoji_loader(train_path, test_path, batch_size, image_size):\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Resize(image_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    rgba_to_rgb,  # Add the custom RGBA to RGB conversion function\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(train_path, transform)\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform)\n",
    "\n",
    "    train_dloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_dloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_dloader, test_dloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_training_loop(dataloader, test_dataloader, opts):\n",
    "    G, D = create_model(opts)\n",
    "    \n",
    "    g_params = G.parameters()\n",
    "    d_params = D.parameters()\n",
    "    \n",
    "    g_optimizer = optim.Adam(g_params, opts.lr, betas=(opts.beta1, opts.beta2))\n",
    "    d_optimizer = optim.Adam(d_params, opts.lr * 2., betas=(opts.beta1, opts.beta2))\n",
    "\n",
    "    train_iter = iter(dataloader)\n",
    "    # test_iter = iter(test_dataloader)\n",
    "\n",
    "    # fixed_noise = sample_noise(opts.batch_size, opts.noise_size)\n",
    "    fixed_noise = torch.randn(opts.batch_size, opts.noise_size, 1, 1)\n",
    "\n",
    "    iter_per_epoch = len(train_iter)\n",
    "    # total_train_iters = opts.train_iters\n",
    "\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "\n",
    "    print(\"Starting Training Loop...\")\n",
    "\n",
    "    try:\n",
    "        for epoch in range(opts.epoch):\n",
    "            for iteration in range(1, opts.train_iters + 1):\n",
    "\n",
    "                # Reset data_iter for each epoch\n",
    "                if iteration % iter_per_epoch == 0:\n",
    "                    train_iter = iter(dataloader)\n",
    "\n",
    "                real_images, _ = next(train_iter)\n",
    "                real_images = to_var(real_images)\n",
    "\n",
    "                for d_i in range(opts.d_train_iters):\n",
    "                    d_optimizer.zero_grad()\n",
    "\n",
    "                    # 1. Compute the discriminator loss on real images\n",
    "                    output = D(real_images).view(-1)\n",
    "                    D_real_loss = torch.mean((output - 1) ** 2) / 2\n",
    "                    D_real_loss.backward()\n",
    "\n",
    "                    # 2. Sample noise\n",
    "                    noise = torch.randn(opts.batch_size, opts.noise_size, 1, 1)\n",
    "\n",
    "                    # 3. Generate fake images from the noise\n",
    "                    fake_images = G(noise)\n",
    "\n",
    "                    # 4. Compute the discriminator loss on the fake images\n",
    "                    output = D(fake_images.detach()).view(-1)\n",
    "                    D_fake_loss = torch.mean(output ** 2) / 2\n",
    "                    D_fake_loss.backward()\n",
    "\n",
    "                    # Update the weights of the discriminator model.\n",
    "                    D_total_loss = D_real_loss + D_fake_loss\n",
    "                    d_optimizer.step()\n",
    "\n",
    "\n",
    "                # Initialize the generator model gradient.\n",
    "                g_optimizer.zero_grad()\n",
    "\n",
    "                # 1. Sample noise\n",
    "                noise = torch.randn(opts.batch_size, opts.noise_size, 1, 1)\n",
    "\n",
    "                # 2. Generate fake images from the noise\n",
    "                fake_images = G(noise)\n",
    "\n",
    "                # 3. Compute the generator loss\n",
    "                output = D(fake_images).view(-1)\n",
    "                G_loss = torch.mean((output - 1) ** 2)\n",
    "                G_loss.backward()\n",
    "                g_optimizer.step()\n",
    "                    \n",
    "                # Save Losses for plotting later\n",
    "                G_losses.append(G_loss.item())\n",
    "                D_losses.append(D_total_loss.item())\n",
    "        \n",
    "            if epoch % opts.log_step == 0:\n",
    "                with torch.no_grad():\n",
    "                    generated_images = G(fixed_noise)\n",
    "                    grid = vutils.make_grid(generated_images, nrow=8, normalize=True)\n",
    "\n",
    "                    plt.figure(figsize=(20,30))\n",
    "                    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "                    plt.title(f'Generated Images in epoch {epoch}')\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Exiting early from training.')\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(G_losses,label=\"G\")\n",
    "    plt.plot(D_losses,label=\"D\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(opts):\n",
    "    dataloader_X, test_dataloader_X = get_emoji_loader(opts.train_path, opts.test_path, opts.batch_size, opts.image_size)\n",
    "\n",
    "    # create_dir(opts.checkpoint_dir)\n",
    "    # create_dir(opts.sample_dir)\n",
    "\n",
    "    gan_training_loop(dataloader_X, test_dataloader_X, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    pass\n",
    "\n",
    "# Load config file\n",
    "args = Options()\n",
    "\n",
    "args.train_path = './emojis/'\n",
    "args.test_path = './emojis/'\n",
    "args.batch_size = 8\n",
    "args.image_size = 32\n",
    "# args.checkpoint_dir = './dir/checkpoint/'\n",
    "# args.sample_dir = './dir/sample/'\n",
    "args.lr = 3.e-5\n",
    "args.beta1 = 0.5\n",
    "args.beta2 = 0.999\n",
    "args.noise_size = 100\n",
    "args.train_iters = 297\n",
    "args.d_train_iters = 5\n",
    "args.epoch = 300\n",
    "args.log_step = 50\n",
    "# args.sample_every = 10\n",
    "# args.checkpoint_every = 10\n",
    "args.d_conv_dim = 32\n",
    "args.g_conv_dim = 32\n",
    "args.spectral_norm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectral_norm = False\n",
    "args.spectral_norm = False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectral_norm = True\n",
    "args.spectral_norm = True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
